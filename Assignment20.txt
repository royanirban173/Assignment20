1. What are the three stages to build the hypotheses or model in machine learning?

Ans.The three stages to build the hypotheses or model in machine learning are as follows

    1.Data Preparation
        a)Data Selection b)Data Processing and c)Data Transformation
    2.Train Data Generation
    3.Algorithm Training

2. What is the standard approach to supervised learning?
Ans. In Supervised Learning the model is trained with examples where the outcomes are known.In doing so the model     captures the features of training data set to perform prediction for unlabeled data.
    Some examples of supervised learning are Regression,Anomaly Detection etc.

3. What is Training set and Test set?

Ans. Training Data is used to train models where the outcomes are known.In doing so the model captures the features of the Training Data which helps it to make predictions on data with unknown outcomes.

Test Data is the data that is used for Tesing the accuracy of Trained Data Models.


4. What is the general principle of an ensemble method and what is bagging and
boosting in ensemble method?
Ans.Ensemble is a supervised learning technique for combining multiple models to produce a single model. Ensemble model works better, when we ensemble models with low correlation.

Bagging (Bootstrap Aggregating)  is an ensemble method. First, we create random samples of the training data set (sub sets of training data set). Then, we build a classifier for each sample. Finally, results of these multiple classifiers are combined using average or majority voting. Bagging helps to reduce the variance error.

Boosting provides sequential learning of the predictors. The first predictor is learned on the whole data set, while the following are learnt on the training set based on the performance of the previous one. It starts by classifying original data set and giving equal weights to each observation. If classes are predicted incorrectly using the first learner, then it gives higher weight to the missed classified observation. Being an iterative process, it continues to add classifier learner until a limit is reached in the number of models or accuracy. Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well. 


5. How can you avoid overfitting ?

We can avoid overfitting by having optimum training data so that noises are not captured in our models.